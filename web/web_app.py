#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FastDatasets Web Interface
åŸºäºGradioçš„æ•°æ®é›†ç”ŸæˆWebç•Œé¢
"""

import os
import sys
import json
import threading
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime

import gradio as gr
from loguru import logger
from rich.console import Console

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(project_root))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from app.core.config import Config
    from app.core.document import DocumentProcessor
    from app.core.dataset import DatasetBuilder
    # from app.core.logger import setup_logger  # loggerå·²åœ¨æ¨¡å—ä¸­é…ç½®
    from app.services.dataset_service import TextSplitter
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print(f"é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"Pythonè·¯å¾„: {sys.path[:3]}")
    sys.exit(1)

# å¯¼å…¥Webæ¨¡å—
from config import web_config
from utils import (
    format_file_size, format_duration, generate_task_id,
    validate_file_format, get_file_info, format_status_message,
    format_results_summary, clean_old_tasks, save_task_state,
    load_task_state, validate_config
)
from status_manager import status_manager

# åˆå§‹åŒ–
console = Console()
config = Config()
# setup_logger()  # loggerå·²åœ¨æ¨¡å—å¯¼å…¥æ—¶é…ç½®

# å…¨å±€å˜é‡
processing_status = load_task_state(web_config.PROJECT_ROOT / "web" / "tasks.json")
processing_results = load_task_state(web_config.PROJECT_ROOT / "web" / "results.json")
processing_lock = threading.Lock()

# å®šæœŸæ¸…ç†æ—§ä»»åŠ¡
def cleanup_old_tasks():
    """æ¸…ç†æ—§ä»»åŠ¡"""
    with processing_lock:
        clean_old_tasks(processing_status, 24)
        clean_old_tasks(processing_results, 24)
        save_task_state(processing_status, web_config.PROJECT_ROOT / "web" / "tasks.json")
        save_task_state(processing_results, web_config.PROJECT_ROOT / "web" / "results.json")

# å¯åŠ¨æ—¶æ¢å¤ä»»åŠ¡çŠ¶æ€
def recover_zombie_tasks():
    """æ¢å¤åƒµå°¸ä»»åŠ¡ - å°†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡æ ‡è®°ä¸ºå¤±è´¥"""
    active_tasks = status_manager.get_active_tasks()
    now = datetime.now()
    
    for task_id, task in active_tasks.items():
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¿è¡Œè¶…è¿‡é…ç½®çš„è¶…æ—¶æ—¶é—´
        elapsed = now - task.start_time
        if elapsed.total_seconds() > web_config.TASK_TIMEOUT:
            logger.warning(f"å‘ç°åƒµå°¸ä»»åŠ¡ {task_id}ï¼Œè¿è¡Œæ—¶é—´: {elapsed}ï¼Œå°†æ ‡è®°ä¸ºå¤±è´¥")
            status_manager.fail_task(
                task_id, 
                f"ä»»åŠ¡è¶…æ—¶ï¼ˆè¿è¡Œ {int(elapsed.total_seconds()//60)} åˆ†é’Ÿï¼‰"
            )

# å¯åŠ¨æ—¶æ‰§è¡Œæ¢å¤
recover_zombie_tasks()

# å¯åŠ¨æ¸…ç†çº¿ç¨‹
cleanup_thread = threading.Thread(target=lambda: [time.sleep(3600), cleanup_old_tasks()], daemon=True)
cleanup_thread.start()

class WebInterface:
    """Webç•Œé¢ä¸»ç±»"""
    
    def __init__(self):
        self.doc_processor = DocumentProcessor()
        self.dataset_generator = DatasetBuilder()
        self.text_splitter = TextSplitter()
        
    def upload_files(self, files) -> Tuple[str, str]:
        """å¤„ç†æ–‡ä»¶ä¸Šä¼ """
        if not files:
            return "âŒ è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶", ""
        # å…¼å®¹å•æ–‡ä»¶ä¸Šä¼ 
        if isinstance(files, str):
            files = [files]
        
        uploaded_files = []
        file_info_list = []
        
        for file_path in files:
            if file_path:
                # éªŒè¯æ–‡ä»¶æ ¼å¼
                if not validate_file_format(file_path, web_config.SUPPORTED_FORMATS):
                    file_ext = Path(file_path).suffix.lower()
                    return f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}\næ”¯æŒçš„æ ¼å¼: {', '.join(web_config.SUPPORTED_FORMATS)}", ""
                
                # è·å–æ–‡ä»¶ä¿¡æ¯
                file_info = get_file_info(file_path)
                if file_info:
                    uploaded_files.append(file_path)
                    file_info_list.append(f"ğŸ“„ {file_info['name']} ({file_info['size_formatted']})")
                else:
                    return f"âŒ æ— æ³•è¯»å–æ–‡ä»¶: {Path(file_path).name}", ""
        
        if not uploaded_files:
            return "âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶", ""
        
        file_list = "\n".join(file_info_list)
        total_size = sum(get_file_info(f).get('size', 0) for f in uploaded_files)
        
        return (f"âœ… æˆåŠŸä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶ (æ€»å¤§å°: {format_file_size(total_size)}):\n{file_list}", 
                json.dumps(uploaded_files))
    
    def start_processing_with_status(self, 
                        files_json: str,
                        output_dir: str,
                        chunk_min_len: int,
                        chunk_max_len: int,
                        output_formats: List[str],
                        enable_cot: bool,
                        llm_concurrency: int,
                        file_concurrency: int,
                        api_key: str,
                        base_url: str,
                        model_name: str,
                        questions_per_chunk: int = 2) -> str:
        """å¼€å§‹å¤„ç†æ–‡æ¡£å¹¶è¿”å›çŠ¶æ€ä¿¡æ¯"""
        result = self.start_processing(
            files_json, output_dir, chunk_min_len, chunk_max_len,
            output_formats, enable_cot, llm_concurrency, file_concurrency,
            api_key, base_url, model_name, questions_per_chunk
        )
        
        # å¦‚æœæˆåŠŸå¼€å§‹å¤„ç†ï¼Œè¿”å›è¯¦ç»†çŠ¶æ€
        if result.startswith("âœ…"):
            return self.get_processing_status()
        else:
            return f"## å¤„ç†å¤±è´¥\n\n{result}"
    
    def start_processing(self, 
                        files_json: str,
                        output_dir: str,
                        chunk_min_len: int,
                        chunk_max_len: int,
                        output_formats: List[str],
                        enable_cot: bool,
                        llm_concurrency: int,
                        file_concurrency: int,
                        api_key: str,
                        base_url: str,
                        model_name: str,
                        questions_per_chunk: int = 2) -> str:
        """å¼€å§‹å¤„ç†æ–‡æ¡£"""
        
        logger.info(f"æ”¶åˆ°å¤„ç†è¯·æ±‚ï¼Œæ–‡ä»¶åˆ—è¡¨: {files_json}")
        
        if not files_json:
            return "âŒ è¯·å…ˆä¸Šä¼ æ–‡ä»¶"
        
        try:
            files = json.loads(files_json)
        except:
            return "âŒ æ–‡ä»¶ä¿¡æ¯è§£æå¤±è´¥"
        
        if not files:
            return "âŒ æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶"
        
        # éªŒè¯é…ç½®å‚æ•°
        config_dict = {
            "chunk_min_len": chunk_min_len,
            "chunk_max_len": chunk_max_len,
            "output_formats": output_formats,
            "llm_concurrency": llm_concurrency,
            "file_concurrency": file_concurrency
        }
        
        is_valid, error_msg = validate_config(config_dict)
        if not is_valid:
            return f"âŒ é…ç½®éªŒè¯å¤±è´¥: {error_msg}"
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = generate_task_id()
        
        # æ›´æ–°é…ç½®
        config.CHUNK_MIN_LEN = chunk_min_len
        config.CHUNK_MAX_LEN = chunk_max_len
        config.OUTPUT_FORMATS = output_formats
        config.ENABLE_COT = enable_cot
        config.MAX_LLM_CONCURRENCY = llm_concurrency
        config.MAX_FILE_CONCURRENCY = file_concurrency
        
        if api_key:
            config.API_KEY = api_key
        if base_url:
            config.BASE_URL = base_url
        if model_name:
            config.MODEL_NAME = model_name
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if not output_dir:
            output_dir = str(project_root / "output")
        
        # ä½¿ç”¨æ–°çš„çŠ¶æ€ç®¡ç†å™¨åˆ›å»ºä»»åŠ¡
        status_manager.create_task(task_id, len(files))
        
        # å¯åŠ¨å¼‚æ­¥å¤„ç†
        def run_async_task():
            import asyncio
            try:
                logger.info(f"å¼€å§‹å¼‚æ­¥å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ - ä»»åŠ¡ID: {task_id}")
                
                # ç®€åŒ–äº‹ä»¶å¾ªç¯ç®¡ç†
                asyncio.run(self._process_files_async(task_id, files, output_dir, questions_per_chunk))
                logger.info(f"ä»»åŠ¡ {task_id} å¤„ç†å®Œæˆ")
                    
            except Exception as e:
                logger.error(f"å¼‚æ­¥å¤„ç†å¤±è´¥: {e}")
                import traceback
                logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                try:
                    status_manager.fail_task(task_id, str(e))
                except Exception as status_error:
                    logger.error(f"é”™è¯¯çŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
        
        thread = threading.Thread(
            target=run_async_task,
            daemon=True
        )
        thread.start()
        
        return f"âœ… å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}ï¼Œå…± {len(files)} ä¸ªæ–‡ä»¶"
    
    async def _process_files_async(self, task_id: str, files: List[str], output_dir: str, questions_per_chunk: int = 2):
        """å¼‚æ­¥å¤„ç†æ–‡ä»¶"""
        logger.info(f"å¼€å§‹å¼‚æ­¥å¤„ç† {len(files)} ä¸ªæ–‡ä»¶ - ä»»åŠ¡ID: {task_id}")
        
        # åˆå§‹åŒ–å¯¼å‡ºæ–‡ä»¶åˆ—è¡¨
        export_files = []
        
        # è·å–è¾“å‡ºæ ¼å¼é…ç½®
        output_formats = config.OUTPUT_FORMATS or ["alpaca"]
        
        try:
            # é˜¶æ®µ1: æ–‡æ¡£å¤„ç†
            logger.info(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€ - ä»»åŠ¡ID: {task_id}")
            status_manager.update_task(task_id, 
                status="processing",
                stage="æ–‡æ¡£è§£æ", 
                message="æ­£åœ¨è§£æå’Œåˆ†å—æ–‡æ¡£...",
                stage_progress=0
            )
            logger.info(f"çŠ¶æ€æ›´æ–°å®Œæˆ - ä»»åŠ¡ID: {task_id}")
            
            results = []
            total_files = len(files)
            
            for i, file_path in enumerate(files):
                logger.info(f"å¤„ç†æ–‡ä»¶ {i+1}/{total_files}: {Path(file_path).name}")
                
                # æ›´æ–°å½“å‰å¤„ç†æ–‡ä»¶
                try:
                    status_manager.update_task(task_id,
                        current_file=Path(file_path).name,
                        progress=int((i / total_files) * 30),  # æ–‡æ¡£å¤„ç†å 30%
                        stage_progress=int((i / total_files) * 100),
                        message=f"æ­£åœ¨å¤„ç†æ–‡æ¡£ {i+1}/{total_files}: {Path(file_path).name}"
                    )
                    logger.info(f"æ–‡ä»¶çŠ¶æ€æ›´æ–°å®Œæˆ - ä»»åŠ¡ID: {task_id}")
                except Exception as status_error:
                    logger.error(f"çŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                
                try:
                    # å¤„ç†å•ä¸ªæ–‡ä»¶
                    logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
                    result = self.doc_processor.process_document(file_path)
                    logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(result) if result else 0} ä¸ªæ–‡æ¡£å—")
                    
                    if result:
                        results.extend(result)
                    
                    # æ›´æ–°å·²å¤„ç†æ–‡ä»¶æ•°
                    try:
                        status_manager.update_task(task_id, processed_files=i + 1)
                        logger.info(f"å·²å¤„ç†æ–‡ä»¶æ•°æ›´æ–°: {i + 1}")
                    except Exception as status_error:
                        logger.error(f"å·²å¤„ç†æ–‡ä»¶æ•°æ›´æ–°å¤±è´¥: {status_error}")
                        
                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    
                    try:
                        status_manager.update_task(task_id, 
                            message=f"å¤„ç†æ–‡ä»¶ {Path(file_path).name} å¤±è´¥: {str(e)}"
                        )
                    except Exception as status_error:
                        logger.error(f"é”™è¯¯çŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
            
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼Œå…±å¾—åˆ° {len(results)} ä¸ªæ–‡æ¡£å—")
            
            # é˜¶æ®µ2: ç”Ÿæˆæ•°æ®é›†
            if results:
                total_chunks = len(results)
                logger.info(f"å¼€å§‹ç”Ÿæˆæ•°æ®é›†ï¼Œå…± {total_chunks} ä¸ªæ–‡æ¡£å—")
                
                # é˜¶æ®µ2.1: ç”Ÿæˆé—®é¢˜
                try:
                    status_manager.update_task(
                        task_id,
                        stage="ç”Ÿæˆé—®é¢˜",
                        message=f"æ­£åœ¨ä¸º {total_chunks} ä¸ªæ–‡æ¡£å—ç”Ÿæˆé—®é¢˜...",
                        progress=35,
                        stage_progress=0,
                        total_chunks=total_chunks,
                        processed_chunks=0
                    )
                    logger.info(f"ç”Ÿæˆé—®é¢˜é˜¶æ®µçŠ¶æ€æ›´æ–°å®Œæˆ")
                except Exception as status_error:
                    logger.error(f"ç”Ÿæˆé—®é¢˜é˜¶æ®µçŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                
                # å¯¼å‡ºæ•°æ®é›†
                output_path = Path(output_dir)
                output_path.mkdir(parents=True, exist_ok=True)
                logger.info(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {output_path}")
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                
                # ä¿å­˜åŸå§‹æ•°æ®
                raw_file = output_path / f"raw_data_{timestamp}.json"
                try:
                    with open(raw_file, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2)
                    logger.info(f"åŸå§‹æ•°æ®å·²ä¿å­˜: {raw_file}")
                except Exception as e:
                    logger.error(f"ä¿å­˜åŸå§‹æ•°æ®å¤±è´¥: {e}")
                
                # ç”Ÿæˆé—®ç­”å¯¹
                qa_pairs = []
                try:
                    # é˜¶æ®µ2.2: ç”Ÿæˆç­”æ¡ˆ
                    try:
                        status_manager.update_task(
                            task_id,
                            stage="ç”Ÿæˆç­”æ¡ˆ",
                            message="æ­£åœ¨ç”Ÿæˆé—®é¢˜ç­”æ¡ˆ...",
                            progress=60,
                            stage_progress=0
                        )
                        logger.info(f"ç”Ÿæˆç­”æ¡ˆé˜¶æ®µçŠ¶æ€æ›´æ–°å®Œæˆ")
                    except Exception as status_error:
                        logger.error(f"ç”Ÿæˆç­”æ¡ˆé˜¶æ®µçŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                    
                    # åˆå§‹åŒ– DatasetBuilder
                    logger.info(f"å¼€å§‹ç”Ÿæˆé—®ç­”å¯¹...")
                    dataset_generator = DatasetBuilder()
                    qa_pairs = await dataset_generator.build_dataset(results)
                    logger.info(f"é—®ç­”å¯¹ç”Ÿæˆå®Œæˆï¼Œå…± {len(qa_pairs)} ä¸ª")
                    
                    # é˜¶æ®µ2.3: ä¿å­˜ç»“æœ
                    try:
                        status_manager.update_task(
                            task_id,
                            stage="ä¿å­˜ç»“æœ",
                            message="æ­£åœ¨ä¿å­˜ç»“æœ...",
                            progress=85,
                            stage_progress=0
                        )
                        logger.info(f"ä¿å­˜ç»“æœé˜¶æ®µçŠ¶æ€æ›´æ–°å®Œæˆ")
                    except Exception as status_error:
                        logger.error(f"ä¿å­˜ç»“æœé˜¶æ®µçŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                    
                except Exception as e:
                    logger.error(f"ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    
                    try:
                        status_manager.update_task(
                            task_id,
                            message=f"ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥: {str(e)}"
                        )
                    except Exception as status_error:
                        logger.error(f"é—®ç­”å¯¹å¤±è´¥çŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                
                # é˜¶æ®µ3: å¯¼å‡ºæ–‡ä»¶
                try:
                    status_manager.update_task(
                        task_id,
                        stage="ä¿å­˜ç»“æœ",
                        message="æ­£åœ¨å¯¼å‡ºæ•°æ®é›†æ–‡ä»¶...",
                        progress=95,
                        stage_progress=0
                    )
                    logger.info(f"å¯¼å‡ºæ–‡ä»¶é˜¶æ®µçŠ¶æ€æ›´æ–°å®Œæˆ")
                except Exception as status_error:
                    logger.error(f"å¯¼å‡ºæ–‡ä»¶é˜¶æ®µçŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                
                # å¯¼å‡ºä¸åŒæ ¼å¼
                try:
                    # ä½¿ç”¨DatasetBuilderçš„export_datasetæ–¹æ³•
                    logger.info(f"å¼€å§‹å¯¼å‡ºæ•°æ®é›†æ–‡ä»¶...")
                    dataset_generator.export_dataset(
                        qa_pairs, 
                        str(output_path), 
                        output_formats, 
                        "json"
                    )
                    logger.info(f"æ•°æ®é›†å¯¼å‡ºå®Œæˆ")
                    
                    # æ„å»ºå¯¼å‡ºæ–‡ä»¶åˆ—è¡¨
                    for fmt in output_formats:
                        file_path = output_path / f"dataset-{fmt}.json"
                        if file_path.exists():
                            export_files.append(str(file_path))
                            logger.info(f"å¯¼å‡ºæ–‡ä»¶ç¡®è®¤: {file_path}")
                        else:
                            logger.warning(f"å¯¼å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                    
                    logger.info(f"æ•°æ®é›†å·²å¯¼å‡ºåˆ°: {output_path}")
                except Exception as e:
                    logger.error(f"å¯¼å‡ºæ•°æ®é›†å¤±è´¥: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                
                # å®Œæˆå¤„ç†
                try:
                    logger.info(f"æ ‡è®°ä»»åŠ¡å®Œæˆ - ä»»åŠ¡ID: {task_id}")
                    # å…ˆæ›´æ–°qa_pairså­—æ®µ
                    status_manager.update_task(task_id, qa_pairs=len(qa_pairs))
                    status_manager.complete_task(
                        task_id,
                        message=f"å¤„ç†å®Œæˆï¼ç”Ÿæˆäº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹"
                    )
                    logger.info(f"ä»»åŠ¡å®ŒæˆçŠ¶æ€æ›´æ–°æˆåŠŸ - ä»»åŠ¡ID: {task_id}")
                    
                    # ä¿å­˜ç»“æœåˆ°å…¨å±€å˜é‡
                    processing_results[task_id] = {
                        "qa_pairs": len(qa_pairs),
                        "export_files": export_files,
                        "raw_file": str(raw_file)
                    }
                    logger.info(f"ç»“æœå·²ä¿å­˜åˆ°å…¨å±€å˜é‡")
                    
                except Exception as status_error:
                    logger.error(f"ä»»åŠ¡å®ŒæˆçŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    
            else:
                logger.warning(f"æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£")
                try:
                    status_manager.fail_task(
                        task_id,
                        "æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡æ¡£"
                    )
                    logger.info(f"å¤±è´¥çŠ¶æ€æ›´æ–°æˆåŠŸ")
                except Exception as status_error:
                    logger.error(f"å¤±è´¥çŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                    
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡ {task_id} å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            try:
                status_manager.fail_task(
                    task_id,
                    f"å¤„ç†å¤±è´¥: {str(e)}"
                )
                logger.info(f"ä¸¥é‡é”™è¯¯çŠ¶æ€æ›´æ–°æˆåŠŸ")
            except Exception as status_error:
                logger.error(f"ä¸¥é‡é”™è¯¯çŠ¶æ€æ›´æ–°å¤±è´¥: {status_error}")
                
        finally:
            logger.info(f"ä»»åŠ¡ {task_id} å¤„ç†æµç¨‹ç»“æŸ")
    
    def get_processing_status(self) -> str:
        """è·å–å¤„ç†çŠ¶æ€"""
        summary = status_manager.get_summary()
        active_tasks = status_manager.get_active_tasks()
        
        if not active_tasks and summary['total_tasks'] == 0:
            return "æš‚æ— å¤„ç†ä»»åŠ¡"

        # å½“å‰ä»»åŠ¡è¯¦æƒ…
        if summary['current_task']:
            task = summary['current_task']
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            elapsed = datetime.now() - task.start_time
            elapsed_str = f"{int(elapsed.total_seconds()//60)}åˆ†{int(elapsed.total_seconds()%60)}ç§’"
            
            # ç®€åŒ–çŠ¶æ€æ˜ å°„
            stage_map = {
                'åˆå§‹åŒ–': 'å¼€å§‹å¤„ç†',
                'æ–‡æ¡£è§£æ': 'å¼€å§‹å¤„ç†', 
                'æ–‡æœ¬åˆ†å—': 'å¼€å§‹å¤„ç†',
                'ç”Ÿæˆé—®é¢˜': 'ç”Ÿæˆé—®é¢˜ä¸­',
                'ç”Ÿæˆç­”æ¡ˆ': 'ç”Ÿæˆç­”æ¡ˆä¸­',
                'ä¿å­˜ç»“æœ': 'å®Œæˆ',
                'ä»»åŠ¡å®Œæˆ': 'å®Œæˆ'
            }
            
            display_stage = stage_map.get(task.stage, task.stage)
            
            # æ ¹æ®ä»»åŠ¡çŠ¶æ€æ˜¾ç¤ºä¸åŒä¿¡æ¯
            if task.status == 'completed':
                # è·å–ç”Ÿæˆçš„é—®ç­”å¯¹æ•°é‡
                qa_count = getattr(task, 'qa_pairs', 0) or task.processed_chunks * 2  # ä¼°ç®—
                return f"**çŠ¶æ€**: å®Œæˆï¼Œå…±ç”Ÿæˆ {qa_count} ä¸ªé—®ç­”å¯¹\n**å¤„ç†æ—¶é—´**: {elapsed_str}"
            elif task.status == 'failed':
                return f"**çŠ¶æ€**: å¤„ç†å¤±è´¥\n**å¤„ç†æ—¶é—´**: {elapsed_str}\n**é”™è¯¯**: {task.error or 'æœªçŸ¥é”™è¯¯'}"
            else:
                return f"**çŠ¶æ€**: {display_stage}\n**å¤„ç†æ—¶é—´**: {elapsed_str}"
        
        return "æš‚æ— æ´»è·ƒä»»åŠ¡"
    
    def get_results_info(self) -> str:
        """è·å–ç»“æœä¿¡æ¯"""
        with processing_lock:
            if not processing_results:
                return "ğŸ“‹ æš‚æ— å®Œæˆçš„ä»»åŠ¡"
            
            results_text = "## å¤„ç†ç»“æœ\n\n"
            
            for task_id, result in processing_results.items():
                results_text += f"### ä»»åŠ¡ {task_id}\n"
                results_summary = format_results_summary(result)
                results_text += f"{results_summary}\n"
                results_text += "---\n\n"
            
            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            save_task_state(processing_results, web_config.PROJECT_ROOT / "web" / "results.json")
            
            return results_text
    
    def export_datasets(self, export_formats: List[str]) -> List[str]:
        """å¯¼å‡ºå·²å®Œæˆçš„æ•°æ®é›†å¹¶è¿”å›æ–‡ä»¶è·¯å¾„åˆ—è¡¨"""
        try:
            # ä»çŠ¶æ€ç®¡ç†å™¨è·å–å·²å®Œæˆä»»åŠ¡
            completed = status_manager.get_completed_tasks()
            if not completed:
                return []
            
            # ä¼˜å…ˆé€‰æ‹©"å·²å®Œæˆ"çš„ä»»åŠ¡ï¼ˆæ’é™¤å¤±è´¥çš„ï¼‰ï¼Œå¦åˆ™å›é€€åˆ°æ‰€æœ‰ç»“æŸçš„ä»»åŠ¡
            completed_only = {k: v for k, v in completed.items() if v.status == 'completed'}
            source_pool = completed_only if completed_only else completed
            
            # é€‰æ‹©æœ€æ–°å®Œæˆçš„ä»»åŠ¡ï¼ˆæŒ‰ end_timeï¼‰
            latest_task_id, latest_task = max(
                source_pool.items(), key=lambda item: item[1].end_time or datetime.min
            )
            
            # ä»»åŠ¡å®Œæˆæ—¶ï¼Œ_process_files_async ä¼šåœ¨è¾“å‡ºç›®å½•ä¸‹ç”Ÿæˆ dataset-alpaca.json / dataset-sharegpt.json
            # éœ€è¦æ ¹æ® raw_file æ¨æ–­è¾“å‡ºç›®å½•
            # å…ˆå°è¯•ä» processing_results ä¸­æ‰¾åˆ°å¯¹åº”ä¿¡æ¯ä½œä¸ºå›é€€
            original_output_dir = None
            with processing_lock:
                latest_result = processing_results.get(latest_task_id)
                if latest_result and latest_result.get('raw_file'):
                    original_output_dir = Path(latest_result['raw_file']).parent
            
            # å¦‚æœæœªèƒ½é€šè¿‡ processing_results æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
            if original_output_dir is None:
                original_output_dir = Path(web_config.DEFAULT_OUTPUT_DIR)
            
            # ä½¿ç”¨é»˜è®¤å¯¼å‡ºç›®å½•
            export_dir = Path("./exports")
            export_dir.mkdir(parents=True, exist_ok=True)
            
            exported_files = []
            for fmt in export_formats:
                try:
                    # åŒ¹é… DatasetBuilder.export_dataset çš„å‘½å: dataset-alpaca.json / dataset-sharegpt.json
                    file_name = f"dataset-{fmt}.json"
                    source_file = original_output_dir / file_name
                    if source_file.exists():
                        import shutil
                        dest_file = export_dir / f"{latest_task_id[:8]}_{source_file.name}"
                        shutil.copy2(source_file, dest_file)
                        exported_files.append(str(dest_file))
                    else:
                        logger.warning(f"æœªæ‰¾åˆ°å¯¼å‡ºæ–‡ä»¶: {source_file}")
                except Exception as e:
                    logger.error(f"å¯¼å‡º {fmt} æ—¶å‡ºé”™: {e}")
                    continue
            
            return exported_files
        except Exception as e:
            logger.error(f"å¯¼å‡ºæ•°æ®é›†æ—¶å‡ºé”™: {e}")
            return []

# åˆ›å»ºWebç•Œé¢å®ä¾‹
web_interface = WebInterface()

def auto_refresh_smart(prev_status: str, prev_results: str):
    """æ™ºèƒ½å®šæ—¶åˆ·æ–°ï¼šä»…åœ¨å†…å®¹å˜åŒ–æ—¶æ›´æ–°ï¼Œå‡å°‘é—ªçƒ
    é€šè¿‡å½’ä¸€åŒ–å¿½ç•¥"ç”¨æ—¶"å­—æ®µçš„ç§’çº§å˜åŒ–ï¼Œé¿å…æ¯æ¬¡tickéƒ½è§¦å‘æ¸²æŸ“ã€‚
    """
    import re

    def normalize_status(s: str) -> str:
        if not s:
            return ""
        # å°† "**ç”¨æ—¶**: Xåˆ†Yç§’" æ›¿æ¢ä¸ºå›ºå®šå ä½ï¼Œé¿å…ç§’çº§æŠ–åŠ¨
        s = re.sub(r"\*\*ç”¨æ—¶\*\*: [^\|\n]+", "**ç”¨æ—¶**: â€”", s)
        return s.strip()

    new_status = web_interface.get_processing_status()
    new_results = web_interface.get_results_info()

    update_status = normalize_status(new_status) != normalize_status(prev_status)
    update_results = new_results != prev_results

    out_status = new_status if update_status else prev_status
    out_results = new_results if update_results else prev_results

    # è¿”å›ï¼šæ˜¾ç¤ºç”¨çš„çŠ¶æ€ä¸ç»“æœï¼Œä»¥åŠç”¨äºä¸‹æ¬¡æ¯”è¾ƒçš„æœ€æ–°çŠ¶æ€ä¸ç»“æœ
    return out_status, out_results, new_status, new_results

def auto_refresh():
    """å®šæ—¶åˆ·æ–°å¤„ç†çŠ¶æ€å’Œç»“æœ"""
    return web_interface.get_processing_status(), web_interface.get_results_info()

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(
        title="FastDatasets - æ™ºèƒ½æ•°æ®é›†ç”Ÿæˆå·¥å…·",
        theme=web_config.get_gradio_theme(),
        css=web_config.get_custom_css()
    ) as interface:
        
        # æ ‡é¢˜å’Œè¯´æ˜
        with gr.Row():
            with gr.Column():
                gr.HTML(
                    """
                    <div style="text-align: center; padding: 20px;">
                        <h1 style="color: #2563eb; margin-bottom: 10px;">FastDatasets</h1>
                        <p style="color: #64748b; font-size: 18px; margin-bottom: 5px;">æ™ºèƒ½æ•°æ®é›†ç”Ÿæˆå·¥å…·</p>
                        <p style="color: #94a3b8; font-size: 14px;">å°†æ–‡æ¡£è½¬æ¢ä¸ºé«˜è´¨é‡çš„LLMè®­ç»ƒæ•°æ®é›†</p>
                    </div>
                    """
                )
        
        # ===== ä¸»åŠŸèƒ½åŒºé‡‡ç”¨ 2Ã—2 ç½‘æ ¼å¸ƒå±€ =====
        # ç¬¬ä¸€è¡Œï¼šä¸Šä¼ æ–‡ä»¶ + å‚æ•°è®¾ç½®
        with gr.Row():
            # å·¦åˆ—ï¼šä¸Šä¼ æ–‡ä»¶
            with gr.Column(scale=1):
                with gr.Accordion("ğŸ“¤ ä¸Šä¼ æ–‡ä»¶", open=True):
                    with gr.Row():
                        file_upload = gr.Files(
                            label="ğŸ“¤ é€‰æ‹©æ–‡æ¡£æ–‡ä»¶ (æ”¯æŒ: PDF, DOCX, TXT, MD)",
                            file_count="multiple",
                            file_types=[".pdf", ".docx", ".txt", ".md"],
                            scale=2
                        )
                        
                        upload_status = gr.Textbox(
                            label="ä¸Šä¼ çŠ¶æ€",
                            interactive=False,
                            lines=3,
                            placeholder="è¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡æ¡£æ–‡ä»¶...",
                            scale=1
                        )
                        
                        files_data = gr.Textbox(
                            visible=False,
                            value=""
                        )
            
            # å³åˆ—ï¼šå‚æ•°è®¾ç½®
            with gr.Column(scale=1):
                with gr.Accordion("âš™ï¸ å‚æ•°è®¾ç½®", open=False):
                    with gr.Row():
                        output_dir = gr.Textbox(
                            label="ğŸ“ è¾“å‡ºç›®å½•",
                            value=web_config.DEFAULT_OUTPUT_DIR,
                            placeholder="ç•™ç©ºä½¿ç”¨é»˜è®¤ç›®å½•",
                            scale=2
                )
                
                        chunk_min_len = gr.Number(
                            label="æœ€å°å—é•¿åº¦",
                            value=web_config.DEFAULT_CHUNK_MIN_LEN,
                            minimum=50,
                            maximum=10000,
                            scale=1
                )
                
                        chunk_max_len = gr.Number(
                            label="æœ€å¤§å—é•¿åº¦",
                            value=web_config.DEFAULT_CHUNK_MAX_LEN,
                            minimum=500,
                            maximum=20000,
                            scale=1
                )
                
                        questions_per_chunk = gr.Number(
                            label="é—®é¢˜æ•°/å—",
                            value=2,
                            minimum=1,
                            maximum=10,
                            scale=1
                )
        
                    # ç¬¬äºŒè¡Œå‚æ•°
                    with gr.Row():
                        output_formats = gr.CheckboxGroup(
                            label="ğŸ“‹ è¾“å‡ºæ ¼å¼",
                            choices=["alpaca", "sharegpt"],
                            value=web_config.DEFAULT_OUTPUT_FORMATS,
                            scale=1
                        )
                
                        enable_cot = gr.Checkbox(
                            label="ğŸ§  å¯ç”¨æ€ç»´é“¾ (CoT)",
                            value=web_config.DEFAULT_ENABLE_COT,
                            scale=1
                        )
                
                        llm_concurrency = gr.Number(
                            label="LLMå¹¶å‘æ•°",
                            value=web_config.DEFAULT_LLM_CONCURRENCY,
                            minimum=1,
                            maximum=50,
                            scale=1
                        )
                
                        file_concurrency = gr.Number(
                            label="æ–‡ä»¶å¹¶å‘æ•°",
                            value=web_config.DEFAULT_FILE_CONCURRENCY,
                            minimum=1,
                            maximum=20,
                            scale=1
                        )
        
            # LLMé…ç½®åŒºåŸŸ - æ¨ªå‘å¸ƒå±€
            with gr.Row():
                api_key = gr.Textbox(
                label="ğŸ”‘ API Key",
                type="password",
                placeholder="ç•™ç©ºä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®",
                scale=2
            )
            
                base_url = gr.Textbox(
                label="ğŸŒ Base URL",
                placeholder="ç•™ç©ºä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®",
                scale=2
            )
            
                model_name = gr.Textbox(
                label="ğŸ¤– æ¨¡å‹åç§°",
                placeholder="ç•™ç©ºä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®",
                scale=1
            )
            
                start_btn = gr.Button(
                "ğŸš€ å¼€å§‹å¤„ç†",
                variant="primary",
                size="lg",
                scale=1
            )
        
        # ç¬¬äºŒè¡Œï¼šå¤„ç†çŠ¶æ€ å’Œ æ•°æ®ä¸‹è½½
        with gr.Row():
            # å·¦åˆ—ï¼šå¤„ç†çŠ¶æ€
            with gr.Column(scale=1):
                with gr.Accordion("ğŸ“ˆ å¤„ç†çŠ¶æ€", open=True):
                    status_display = gr.Markdown(
                        "ğŸ“‹ ç­‰å¾…ä»»åŠ¡å¼€å§‹...",
                        elem_classes=["progress-section"]
                    )
        
            # å³åˆ—ï¼šæ•°æ®ä¸‹è½½
            with gr.Column(scale=1):
                with gr.Accordion("ğŸ“¥ æ•°æ®ä¸‹è½½", open=False):
                    with gr.Row():
                        export_format = gr.CheckboxGroup(
                            choices=["alpaca", "sharegpt"],
                            value=["alpaca"],
                            label="ğŸ“‚ å¯¼å‡ºæ ¼å¼",
                            scale=1
                        )
            
                    export_btn = gr.Button(
                        "ğŸ“¥ å¯¼å‡ºæ•°æ®é›†",
                        variant="primary",
                        scale=1
                    )
            
                    downloaded_files = gr.Files(
                        label="ä¸‹è½½å¯¼å‡ºçš„æ–‡ä»¶",
                        interactive=False
                    )
        
        # å¤„ç†ç»“æœæ¨¡å—å·²ç§»é™¤

        # çŠ¶æ€å­˜å‚¨
        status_state = gr.State("")

        
        # äº‹ä»¶ç»‘å®š
        file_upload.change(
            fn=web_interface.upload_files,
            inputs=[file_upload],
            outputs=[upload_status, files_data]
        )
        
        # å¼€å§‹å¤„ç†æŒ‰é’®äº‹ä»¶
        start_btn.click(
            fn=web_interface.start_processing_with_status,
            inputs=[
                files_data, output_dir, chunk_min_len, chunk_max_len,
                output_formats, enable_cot, llm_concurrency, file_concurrency,
                api_key, base_url, model_name, questions_per_chunk
            ],
            outputs=[status_display]
        )
        
        # å®šæ—¶åˆ·æ–°çŠ¶æ€
        refresh_timer = gr.Timer(value=2.0)
        refresh_timer.tick(
            fn=lambda prev_status: web_interface.get_processing_status(),
            inputs=[status_state],
            outputs=[status_display]
        )
        
        # å¯¼å‡ºæŒ‰é’®äº‹ä»¶
        export_btn.click(
            fn=web_interface.export_datasets,
            inputs=[export_format],
            outputs=[downloaded_files]
        )
    
    return interface

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='FastDatasets Web Interface')
    parser.add_argument('--port', type=int, default=web_config.PORT, help='æœåŠ¡ç«¯å£å·')
    parser.add_argument('--host', type=str, default=web_config.HOST, help='æœåŠ¡ä¸»æœºåœ°å€')
    parser.add_argument('--share', action='store_true', help='å¯ç”¨å…¬ç½‘è®¿é—®')
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = create_interface()
    
    # å¯åŠ¨æœåŠ¡
    # ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡æ˜¯ç¯å¢ƒå˜é‡ï¼Œæœ€åæ˜¯é…ç½®æ–‡ä»¶
    server_port = args.port or int(os.getenv("GRADIO_SERVER_PORT", web_config.PORT))
    server_host = args.host or web_config.HOST
    share = args.share or web_config.SHARE
    
    interface.launch(
        server_name=server_host,
        server_port=server_port,
        share=share,
        show_error=True,
        quiet=False,
        inbrowser=web_config.INBROWSER,
        show_api=False
    )